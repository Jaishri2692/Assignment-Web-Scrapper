{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044ce29a-a821-4b16-8063-ac03c1473b73",
   "metadata": {},
   "source": [
    "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "\n",
    "Web scraping (also called web data extraction or data scraping) is an automated process that extracts data from a website and exports it in a structured format.\n",
    "\n",
    "Some of the main use cases of web scraping include price monitoring, price intelligence, news monitoring, lead generation, and market research among many others.\n",
    "\n",
    "In general, web scraping is used by people and businesses who want to make use of publicly available web data to generate valuable insights and make smarter decisions.\n",
    "\n",
    "Web scraping is especially useful if the public website you want to get data from doesn’t have an API, or only provides limited access to web data. \n",
    "\n",
    "# Three areas where Web Scraping is used to get data\n",
    "\n",
    "1. Price Intelligence\n",
    "In our experience, price intelligence is the biggest use case for web scraping.\n",
    "\n",
    "Extracting product and pricing information from e-commerce websites, then turning it into intelligence is an important part of modern e-commerce companies that want to make better pricing/marketing decisions based on data.\n",
    "\n",
    "# Web pricing data and price intelligence benefits:\n",
    "\n",
    "i.   Dynamic pricing\n",
    "ii.  Revenue optimization\n",
    "iii. Competitor monitoring\n",
    "iv.  Product trend monitoring\n",
    "v.   Brand and MAP compliance\n",
    "\n",
    "2. Market research\n",
    "Market research is critical – and should be driven by the most accurate information available. With data scraping, you get high quality, high volume, and highly insightful web-scraped data of every shape and size is fueling market analysis and business intelligence across the globe.\n",
    "\n",
    "i.   Market trend analysis\n",
    "ii.  Market pricing\n",
    "iii. Optimizing point of entry\n",
    "iv.  Research & development\n",
    "v.   Competitor monitoring\n",
    "\n",
    "3. News & content monitoring\n",
    "Modern media can create outstanding value or an existential threat to your business - in a single news cycle.\n",
    "\n",
    "If you’re a company that depends on timely news analyses, or a company that frequently appears in the news, web scraping news data is the ultimate solution for monitoring, aggregating, and parsing the most critical stories from your industry.\n",
    "\n",
    "i.   Investment Decision Making\n",
    "ii.  Online Public Sentiment Analysis\n",
    "iii. Competitor Monitoring\n",
    "iv.  Political Campaigns\n",
    "v.   Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9056d88-c1a7-43b0-ad9c-591a50102bda",
   "metadata": {},
   "source": [
    "# Q2. What are the different methods used for Web Scraping?\n",
    "\n",
    "\n",
    "1. Using a web scraping software\n",
    "Web Scraping software falls under 2 categories. First, which can be locally installed in your computer and second, which runs in the cloud (browser based). WebHarvy, OutWit Hub, Visual Web Ripper etc. are examples of web scraping software which can be installed in your computer, whereas import.io, Mozenda, ParseHub, OctoParse etc. are examples of cloud data extraction platforms.\n",
    "\n",
    "\n",
    "2. By writing code or by hiring a developer\n",
    "You can hire a developer to build custom data extraction software for your specific requirement. The developer can in-turn make use of web scraping APIs or libraries. For example, apify.com lets you easily get APIs to scrape data from any website. Beautiful Soup is a Python library which helps you parse data out of HTML code behind web pages.\n",
    "\n",
    "# Types of Scraping Tools\n",
    "\n",
    "# i. Web Scraper Browser Extension – \n",
    "This is a Chrome browser extension. Users can create a sitemap detailing how to crawl a website and what data to extract.  \n",
    "# ii. Self-Built Web Scrapers – \n",
    "You have the option to build your own web scraper and fine-tune it to your specifications. This route requires advanced programming skills, including the ability to web scrape with Python.\n",
    "# iii. Cloud Web Scrapers – \n",
    "These web scrapers run on off-site servers, which the service provider usually provides. This avoids burdening your computer while the scraper is gathering and parsing data. Cloud-based solutions typically offer more advanced (albeit expensive) features. \n",
    "# iv. Web Scraping Software – \n",
    "These web scrapers contain advanced features, like IP rotation, JavaScript execution, proxy management, and anti-bot technology workarounds. \n",
    "# v. User Interface Web Scrapers – \n",
    "These web scrapers have sophisticated user interfaces that allow the rendering of the website completely. The user simply has to click on the data they want to scrape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a95308-1c80-4ca7-8418-12390ebdda45",
   "metadata": {},
   "source": [
    "# Q3. What is Beautiful Soup? Why is it used?\n",
    "\n",
    "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.\n",
    "\n",
    "Beautiful Soup is a great tool for extracting very specific information from large unstructured raw Data, and also it is very fast and handy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c41ff-8e58-4470-9685-6c25d388ade5",
   "metadata": {},
   "source": [
    "# Q4. Why is flask used in this Web Scraping project?\n",
    "\n",
    "Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0cde6e-0fb2-4871-8265-d9ddee6a2702",
   "metadata": {},
   "source": [
    "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "Below two AWS services used in this project:\n",
    "\n",
    "# 1. AWS code pipeline:\n",
    "AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.\n",
    "\n",
    "The pipeline as code model creates automated processes that help developers build applications more efficiently. Having everything documented in a source repository allows for greater visibility and collaboration so that everyone can continually improve processes.\n",
    "\n",
    "The following are the valid action categories in CodePipeline:\n",
    "Source.\n",
    "Build.\n",
    "Test.\n",
    "Deploy.\n",
    "Approval.\n",
    "Invoke.\n",
    "\n",
    "# 2. AWS Elastic Beanstalk\n",
    "AWS Elastic Beanstalk automatically launches an environment and creates and configures the AWS resources needed to run your code. \n",
    "\n",
    "Elastic Beanstalk is a service for deploying and scaling web applications and services. \n",
    "\n",
    "Upload your code and Elastic Beanstalk automatically handles the deployment—from capacity provisioning, load balancing, and auto scaling to application health monitoring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
